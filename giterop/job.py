"""
A Job is generated by comparing a list of specs with the last known state of the system.
Job runs tasks, each of which has a configuration spec that is executed on the running system
Each task tracks and records its modifications to the system's state
"""

import collections
import datetime
import types
from .support import Status, Priority, AttributeManager, ResourceChanges
from .util import GitErOpError, GitErOpTaskError, mergeDicts
from .runtime import ConfigChange, Resource
from .configurator import TaskView, ConfiguratorResult, Dependency, ConfigurationSpec
# from .plan import Plan XXX

import logging
logger = logging.getLogger('giterop')
logger.setLevel(logging.DEBUG)

class TaskRequest(object):
  def __init__(self, configSpec, resource, persist, required):
    self.configSpec = configSpec
    self.target = resource
    self.persist = persist
    self.required = required

class JobRequest(object):
  def __init__(self, resources, errors):
    self.resources = resources
    self.errors = errors

class ConfigTask(ConfigChange, TaskView, AttributeManager):
  """
    receives a configSpec and a target node instance
    instantiates and runs Configurator
    updates Configurator's target's status and lastConfigChange
  """
  def __init__(self, job, configSpec, target, previousConfigChangeId=None, parentId=None, reason = None):
    ConfigChange.__init__(self)
    TaskView.__init__(self, job.runner.manifest, configSpec, target, reason)
    AttributeManager.__init__(self)
    self.parentId = parentId or job.changeId
    self.changeId = self.parentId
    self.startTime = job.startTime or datetime.datetime.now()
    self.errors = []
    self.dryRun = job.dryRun
    self.generator = None
    self.job = job
    self.changeList = []
    self.result = None
    self.previousConfigChangeId = previousConfigChangeId

    # set the attribute manager on the root resource
    # XXX refcontext in attributeManager should define $TARGET $HOST etc.
    #self.configuratorResource.root.attributeManager = self
    self.target.root.attributeManager = self

    # XXX set self.configSpec.target.readyState = 'pending'
    self.configurator = self.configSpec.create()

  def priority():
    doc = "The priority property."
    def fget(self):
      if self._priority is None:
        return self.configSpec.shouldRun()
      else:
        return self._priority
    def fset(self, value):
      self._priority = value
    def fdel(self):
      del self._priority
    return locals()
  priority = property(**priority())

  def startRun(self):
    if self.dryRun:
      self.generator = self.configurator.dryRun(self)
    else:
      self.generator = self.configurator.run(self)
    assert isinstance(self.generator, types.GeneratorType)

  def send(self, change):
    result = None
    try:
      result = self.generator.send(change)
    finally:
      # serialize configuration changes
      self.commitChanges()
    return result

  def start(self):
    self.startRun()

  def _setConfigStatus(self, config, result):
    statusChanged = config.localStatus != result.readyState
    if result.configChanged is None:
      # not set so try to deduce
      if self.reason in ['config changed', 'all']:
        configChanged = statusChanged or self.changeList or self.dependenciesChanged
      else: # be conservative, assume the worse
        configChanged = True
    else:
      # setting result.configChanged will override change detection
      configChanged = result.configChanged

    if configChanged:
      config._lastConfigChange = self.changeId
    if statusChanged:
      config.localStatus = result.readyState
    logger.debug("task %s statusChanged: %s configChanged %s", self, statusChanged and config.localStatus, configChanged and config._lastConfigChange)

  def processResult(self, result):
    """
    Update the target instance with the result.

    `result.applied` indicates this configuration is active
    (essentially, the owner of the instance's configuration)
    `result.modified` indicates if a "physical" change to this system was made.
     (All combinations of these two are permissible -- modified and not applied
     means changes were made to the system that couldn't be undone.
    """
    instance = self.target
    if result.modified:
      instance._lastStateChange = self.changeId

    if result.applied:
      assert result.readyState and result.readyState != Status.notapplied, result.readyState
      self._setConfigStatus(instance, result)
    else:
      # configurator wasn't able to apply so leave the instance state as is
      # except in the case where explicitly set another status
      # e.g. if it left the instance in an error state
      if result.readyState and result.readyState != Status.notapplied:
        instance.localStatus = result.readyState

  def finished(self, result):
    if self.generator:
      self.generator.close()
      self.generator = None

    # don't set the changeId until we're finish so that we have a higher changeid
    # than nested tasks and jobs that ran (avoids spurious config changed tasks)
    self.changeId = self.job.runner.incrementChangeId()
    #XXX2 if attributes changed validate using attributesSchema
    #XXX2 Check that configuration provided the metadata that it declared it would provide (see findMissingProvided)
    self.processResult(result)
    resource = self.target

    if (result.applied or result.modified) and self.changeList:
      # merge changes together (will be saved with changeset)
      changes = self.changeList
      accum = changes.pop(0)
      while changes:
        accum = mergeDicts(accum, changes.pop(0))

      self._resourceChanges.updateChanges(accum, self.statuses, resource)

    self.result = result
    if result.readyState:
      self.localStatus = result.readyState
    return self

  def commitChanges(self):
    """
    This can be called multiple times if the configurator yields multiple times.
    Save the changes made each time.
    """
    changes = AttributeManager.commitChanges(self)
    self.changeList.append(changes)
    return changes

  # XXX
  def hasParametersChanged(self):
    """
    Evaluate configuration spec's parameters and compare with the current paramenters' values
    """
    specParams = self.configSpec.parameters
    # XXX need to load last change
    _parameters = self.lastConfigChange and self.lastConfigChange.parameters # serialized, not live
    if _parameters is None:
      # if _parameters were not set
      # return True if parameters have been declared
      return not not specParams

    if set(specParams) != set(_parameters):
      return True #params were added or removed

    # XXX3 not all parameters need to be live
    # add an optional liveParameters attribute to config spec to specify which ones to check

    # treat each parameter as a dependency
    # the expression is the name of parameter
    # val will be the last saved parameter values, re-evaluate the spec'd params and compare them
    # note: external values will be compared
    return any(Dependency(".::" + name, val, name=name).hasChanged(self)
                                  for name, val in _parameters.items())

  def hasDependenciesChanged(self):
    return any(d.hasChanged(self) for d in self.dependencies.values())

  def refreshDependencies(self):
    for d in self.dependencies.values():
      d.refresh(self)

  def __repr__(self):
    return "ConfigTask(%s:%s %s)" % (
      self.target,
      self.configSpec.name,
      self.reason or 'unknown'
    )

class JobOptions(object):
  """
  Options available to select which tasks are run, e.g. read-only

  does the config apply to the action?
  is it out of date?
  is it in a ok state?
  """
  defaults = dict(
    parentJob=None,
    startTime=None,
    out=None,

    resource=None,
    resources=None,
    template=None,
    useConfigurator=False,

    # default options:
    add=True, # add new templates
    update=True, # run configurations that whose spec has changed but don't require a major version change
    repair="error", # or 'degraded' or "notapplied" or "none", run configurations that are not operational and/or degraded

    upgrade=False, # run configurations with major version changes or whose spec has changed
    all=False, # (re)run all configurations
    verify=False, # XXX3 discover first and set status if it differs from expected state
    readOnly=False, # only run configurations that won't alter the system
    dryRun=False, # XXX2
    requiredOnly=False,
    revertObsolete=False, #revert

    append=None,
    replace=None,
    cmdline=None
    )

  def __init__(self, **kw):
    options = self.defaults.copy()
    options.update(kw)
    self.__dict__.update(options)

class Job(ConfigChange):
  """
  runs ConfigTasks and Jobs
  """

  def __init__(self, runner, rootResource, plan, jobOptions):
    super(Job, self).__init__(Status.ok)
    assert isinstance(jobOptions, JobOptions)
    self.__dict__.update(jobOptions.__dict__)
    self.jobOptions = jobOptions
    self.runner = runner
    self.plan = plan
    self.rootResource = rootResource
    self.jobRequestQueue = []
    self.unexpectedAbort = None
    # note: tasks that never run will all share this changeid
    self.changeId = runner.incrementChangeId()
    self.parentId = self.parentJob.changeId if self.parentJob else None
    self.workDone = collections.OrderedDict()

  def createTask(self, configSpec, target, parentId=None, reason=None):
    # XXX2 if 'via'/runsOn set, create remote task instead
    task = ConfigTask(self, configSpec, target, target.lastConfigChange, parentId, reason = reason)
    return task

  def getCandidateTasks(self):
    # XXX plan might call job.runJobRequest(configuratorJob) before yielding
    for (configSpec, target, reason) in self.plan.executePlan():
      if self.runner.isConfigAlreadyHandled(configSpec):
        # configuration may have premptively run while executing another task
        continue

      yield self.createTask(configSpec, target, reason=reason)

  def run(self):
    for task in self.getCandidateTasks():
      self.runner.addWork(task)
      if not self.shouldRunTask(task):
        continue

      logger.info("running task %s", task)
      self.runTask(task)

      if self.shouldAbort(task):
        return self.rootResource

    # the only jobs left will be those that were added to resources already iterated over
    # and were not yielding inside runTask
    while self.jobRequestQueue:
      jobRequest = self.jobRequestQueue[0]
      job = self.runJobRequest(jobRequest)
      if self.shouldAbort(job):
        return self.rootResource

    # XXX
    # if not self.parentJob:
    #   # create a job that will re-run configurations whose parameters or runtime dependencies have changed
    #   # ("config changed" tasks)
    #   # XXX3 check for orphaned resources and mark them as orphaned
    #   #  (a resource is orphaned if it was added as a dependency and no longer has dependencies)
    #   #  (orphaned resources can be deleted by the configuration that created them or manages that type)
    #   maxloops = 10 # XXX3 better loop detection
    #   for count in range(maxloops):
    #     jobOptions = JobOptions(parentJob=self, repair='none')
    #     plan = Plan(self.rootResource, self.runner.manifest.tosca, jobOptions)
    #     job = Job(self.runner, self.rootResource, plan, jobOptions)
    #     job.run()
    #     # break when there are no more tasks to run
    #     if not len(job.workDone) or self.shouldAbort(job):
    #       break
    #   else:
    #     raise GitErOpError("too many final dependency runs")

    return self.rootResource

  def runJobRequest(self, jobRequest):
    self.jobRequestQueue.remove(jobRequest)
    resourceNames=[r.name for r in jobRequest.resources]
    jobOptions = JobOptions(parentJob=self, repair='none',
      resources=resourceNames)
    plan = Plan(self.rootResource.root, self.runner.manifest.tosca, jobOptions)
    childJob = Job(self.runner, self.rootResource.root, plan, jobOptions)
    assert childJob.parentJob is self
    childJob.run()
    return childJob

  def shouldRunTask(self, task):
    """
    Checked at runtime right before each task is run

    * check "when" conditions to see if it should be run
    * check task if it should be run
    """
    try:
      priority = task.configurator.shouldRun(task)
    except Exception:
      #unexpected error don't run this
      GitErOpTaskError(task, "shouldRun failed")
      return False

    task.priority = priority
    return priority > Priority.ignore

  def canRunTask(self, task):
    """
    Checked at runtime right before each task is run

    * validate parameters
    * check pre-conditions to see if it can be run
    * check task if it can be run
    """
    try:
      canRun = False
      reason = ''
      if task.configSpec.validateParameters():
        preErrors = task.configSpec.getInvalidPreconditions(task.target)
        if not preErrors:
          if task.configSpec.canRun():
              if task.configurator.canRun(task):
                canRun = True
              else:
                reason = 'configurator declined'
          else:
            reason = 'configSpec declined'
        else:
          reason = 'invalid precondition: %s' % preErrors
      else:
        reason = 'invalid inputs'
    except Exception:
      GitErOpTaskError(task, "canRun failed")
      canRun = False

    if canRun:
      return True
    else:
      logger.info("could not run task %s: %s", task, reason)
      return False

  def shouldAbort(self, task):
    return False #XXX3

  def summary(self):
    def format(name, task):
      required = '[required]' if task.required else ''
      return "%s: %s:%s: %s" % (name, required, task.status.name, task.result)

    line1 = 'Job %s completed: %s. Tasks:\n    ' % (self.changeId, self.status.name)
    tasks = '\n    '.join(format(name, task) for name, task in self.workDone.items())
    return (line1 + tasks)

  def getOperationalDependencies(self):
    # XXX3 this isn't right, root job might have too many and child job might not have enough
    # plus dynamic configurations probably shouldn't be included if yielded by a configurator
    for task in self.workDone.values():
      yield task

  def runTask(self, task):
    """
    During each task run:
    * Notification of metadata changes that reflect changes made to resources
    * Notification of add or removing dependency on a resource or properties of a resource
    * Notification of creation or deletion of a resource
    * Requests a resource with requested metadata, if it doesn't exist, a task is run to make it so
    (e.g. add a dns entry, install a package).
    """
    # XXX3 need a way for configurator to declare that is the manager of a particular resource or type of resource or metadata so we know to handle that request
    # XXX3 recursion or loop detection
    if not self.canRunTask(task):
      return task.finished(ConfiguratorResult(False, False))

    task.start()
    change = None
    while True:
      try:
        result = task.send(change)
      except Exception:
        GitErOpTaskError(task, "configurator.run failed")
        # assume the worst
        return task.finished(ConfiguratorResult(True, True, Status.error))
      if isinstance(result, TaskRequest):
        subtask = self.createTask(result.configSpec, result.target, self.changeId)
        self.runner.addWork(subtask)
        change = self.runTask(subtask) # returns a configuration
      elif isinstance(result, JobRequest):
        job = self.runJobRequest(result)
        change = job
      elif isinstance(result, ConfiguratorResult):
        retVal = task.finished(result)
        logger.info("finished running task %s: %s; %s", task, task.target.status, result)
        return retVal
      else:
        GitErOpTaskError(task, 'unexpected result from configurator')
        return task.finished(ConfiguratorResult(True, True, Status.error))

class Runner(object):
  def __init__(self, manifest, localEnv=None):
    # if not localEnv:
    #   localEnv = LocalEnv()
    if localEnv:
      localEnv.addManifest(manifest)
    self.manifest = manifest
    assert self.manifest.tosca
    self.lastChangeId = manifest.lastChangeId
    self.currentJob = None

  def addWork(self, task):
    key = task.configSpec.name
    self.currentJob.workDone[key] = task
    task.job.workDone[key] = task

  def isConfigAlreadyHandled(self, configSpec):
    return configSpec.name in self.currentJob.workDone

  def createJob(self, joboptions):
    """
    Selects task to run based on job options and starting state of manifest
    """
    root = self.manifest.getRootResource()
    assert self.manifest.tosca
    plan = Plan(root, self.manifest.tosca, joboptions)
    return Job(self, root, plan, joboptions)

  def incrementChangeId(self):
    self.lastChangeId +=1
    return self.lastChangeId

  def run(self, jobOptions=None):
    """
    """
    if jobOptions is None:
      jobOptions = JobOptions()
    job = self.createJob(jobOptions)
    self.currentJob = job
    try:
      job.run()
    except Exception:
      job.localStatus = Status.error
      job.unexpectedAbort = GitErOpError("unexpected exception while running job", True)
    self.currentJob = None
    self.manifest.saveJob(job)
    return job

class Plan(object):
  """
  create:  template or unapplied resource
  upgrade: resource
  delete:  resource
  check:   resource

  options:
  --append with create to avoid error if exists
  in the future, should run with previous command on this resource or template
  use:configurator use that configurator
  """
  rootConfigurator = None # XXX3

  def __init__(self, root, toscaSpec, jobOptions):
    self.jobOptions = jobOptions
    self.root = root
    self.tosca = toscaSpec
    assert self.tosca

  def findResourcesFromTemplate(self, nodeTemplate):
    for resource in self.root.getSelfAndDescendents():
      if resource.template.name == nodeTemplate.name:
        yield resource

  def findResourceFromTemplate(self, nodeTemplate):
    for resource in self.root.getSelfAndDescendents():
      if resource.template.name == nodeTemplate.name:
        return resource

  def createResource(self, template):
    # XXX create capabilities and requirements too?
    return Resource(template.name, template=template, parent=self.root)

  def getConfigurationSpecFromInterface(self, iDef):
    '''implementation can either be a named artifact (including a python configurator class),
      configurator node template, or a file path'''

    implementation = iDef.implementation
    if isinstance(implementation, dict):
      implementation = implementation.get('primary')
    configuratorTemplate = self.tosca.configurators.get(implementation)
    if configuratorTemplate:
      attributes = configuratorTemplate.properties
      kw = { k : attributes[k] for k in set(attributes) & set(ConfigurationSpec.getDefaults()) }
      if 'parameters' not in kw:
        kw['parameters'] = iDef.inputs
      if 'className' not in kw:
        kw['className'] = configuratorTemplate.getInterfaces()[0].implementation
      return ConfigurationSpec(configuratorTemplate.name, iDef.name, **kw)
    else:
      # for now assume its a configurator class
      # XXX: see if its a artifact, if its a executable file, create a ShellConfigurator
      return ConfigurationSpec(implementation, iDef.name, className=implementation, parameters = iDef.inputs)

  def findImplementation(self, interfaceType, operation, template):
    for iDef in template.getInterfaces():
      if iDef.type == interfaceType and iDef.name == operation:
        return self.getConfigurationSpecFromInterface(iDef)
    return None

  def executePlan(self):
    """
    yields configSpec, target, reason
    """
    opts = self.jobOptions
    if opts.template:
      template = self.tosca.getTemplate(opts.template)
    else:
      # XXX randoming picking the first one
      template = self.tosca.nodeTemplates and [
        t for t in self.tosca.nodeTemplates.values()
          if not t.isCompatibleType(self.tosca.ConfiguratorType)][0]
    if not template:
      raise GitErOpError('template not found %s' % template)

    if opts.add:
      skipAdd = False
      # XXX NodeInstance instead to include relationships
      resource = self.findResourceFromTemplate(template)
      if resource:
        if resource.status != Status.notapplied:
          if not opts.append:
            if not opts.upgrade:
              raise GitErOpError('resource %s already created for template %s (run with --upgrade)' % (resource.name, template.name))
            else:
              skipAdd = True
      else:
        # XXX NodeInstance instead to include relationships
        resource = self.createResource(template)
      if not skipAdd:
        yield self.generateConfiguration('create', resource, 'add', opts.useConfigurator)

    if opts.upgrade:
      if not resource:
        resource = self.findResourceFromTemplate(template)
      if not resource or resource.status != Status.notapplied:
        yield self.generateConfiguration('create', resource, 'upgrade', opts.useConfigurator)
      else:
        yield self.generateConfiguration('configure', resource, 'upgrade', opts.cmdline, opts.useConfigurator)

  def generateConfiguration(self, action, resource, reason=None, cmdLine=None, useConfigurator=None):
    # XXX update joboptions, useConfigurator
    if cmdLine:
      params = dict(command=cmdLine)
      configSpec = ConfigurationSpec('cmdline', action, className='ShellConfigurator', parameters = params)
    else:
      configSpec = self.findImplementation('Standard', action, resource.template)
    if not configSpec:
      raise GitErOpError('unable to find an implementation to "%s" "%s"' % (action, resource.name) )
    return (configSpec, resource, reason or action)
